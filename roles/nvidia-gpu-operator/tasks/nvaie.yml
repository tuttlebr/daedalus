---
# vGPU support
- name: Disable GPU Operator MIG Manager on NVIDIA AI Enterprise
  set_fact:
    gpu_operator_enable_migmanager: false

- name: Create namespace for GPU Operator resources
  shell: set -o pipefail && kubectl create namespace {{ gpu_operator_namespace }} -o yaml --dry-run=client | kubectl apply -f -
  changed_when: false

- name: create local directory for gpu operator configuration
  file:
    path: "{{ gpu_operator_grid_config_dir }}"
    state: directory
    owner: "root"
    group: "root"
    mode: "0700"

- name: add gridd.conf config file from template
  template:
    src: "gridd.conf"
    dest: "{{ gpu_operator_grid_config_dir }}/gridd.conf"
    owner: "root"
    group: "root"
    mode: "0600"

- name: add client_configuration_token.tok token file from template
  template:
    src: "client_configuration_token.tok"
    dest: "{{ gpu_operator_grid_config_dir }}/client_configuration_token.tok"
    owner: "root"
    group: "root"
    mode: "0600"

- name: Create a docker gridd.conf license configmap
  shell: set -o pipefail && kubectl create configmap licensing-config -n "{{ gpu_operator_namespace }}" --from-file="{{ gpu_operator_grid_config_dir }}/gridd.conf" --from-file="{{ gpu_operator_grid_config_dir }}/client_configuration_token.tok" -o yaml --dry-run=client | kubectl apply -f -
  changed_when: false

- name: Create a docker secret for private GPU Operator containers
  shell: set -o pipefail && kubectl create secret docker-registry registry-secret --docker-server='{{ gpu_operator_driver_registry }}' --docker-username='{{ gpu_operator_registry_username }}' --docker-password={{ gpu_operator_registry_password }} --docker-email={{ gpu_operator_registry_email }} -n {{ gpu_operator_namespace }} -o yaml --dry-run=client | kubectl apply -f -
  changed_when: false

# While we would prefer to use the Ansible helm module, it's broken! :-(
# See https://github.com/ansible/ansible/pull/57897
# Unfortunately this will not be fixed until Ansible 2.10 which is not yet released.
# So for now we will run /usr/local/bin/helm commands directly...
- name: install NVAIE gpu-operator helm repo
  command: /usr/local/bin/helm repo add nvaie "{{ gpu_operator_nvaie_helm_repo }}" --username='{{ gpu_operator_registry_username }}' --password="{{ gpu_operator_registry_password }}"
  changed_when: false

- name: update helm repos
  command: /usr/local/bin/helm repo update
  changed_when: false

# TODO: This is no longer the cae # XXX: This currently installs into the default namespace, as per the GPU Operator docs
# https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html
- name: install nvidia gpu operator
  command: /usr/local/bin/helm upgrade --install "{{ gpu_operator_release_name }}" "{{ gpu_operator_nvaie_chart_name }}" --version "{{ gpu_operator_chart_version }}" --namespace "{{ gpu_operator_namespace }}" --create-namespace --set driver.licensingConfig.configMapName=licensing-config --set driver.licensingConfig.nlsEnabled="{{ gpu_operator_nvaie_nls_enabled }}"  --set driver.imagePullSecrets[0]="registry-secret" --set operator.imagePullSecrets[0]="registry-secret" --set driver.repository="{{ gpu_operator_driver_registry }}" --set driver.version="{{ gpu_operator_driver_version }}"  --set mig.strategy="{{ k8s_gpu_mig_strategy }}"  --set driver.enabled="{{ gpu_operator_enable_driver }}" --set toolkit.enabled="{{ gpu_operator_enable_toolkit }}" --set dcgm.enabled="{{ gpu_operator_enable_dcgm }}" --set migManager.enabled="{{ gpu_operator_enable_migmanager }}" --wait
  changed_when: false
